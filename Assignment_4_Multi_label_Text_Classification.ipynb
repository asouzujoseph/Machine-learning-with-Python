{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asouzujoseph/Machine-learning-with-Python/blob/main/Assignment_4_Multi_label_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnV4CTTPNO7"
      },
      "source": [
        "# SCS 3253: Machine Learning\n",
        "> Assignment 4: Multi-label Text Classification\n",
        "\n",
        "> Assigned: July 19th, 2022\n",
        "\n",
        "> **Due Date: August 2nd, 2022, 6:30pm EST**\n",
        "\n",
        "Please review the assignment early so you have ample opportunity to ask clarifying questions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLx-XyaePRlq"
      },
      "source": [
        "### Your name & student number:\n",
        "\n",
        "<pre> Name: Nnamdi joseph Asouzu </pre>\n",
        "\n",
        "<pre> Student Number: X487118 </pre>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE2CN5nsPRbz"
      },
      "source": [
        "# Instructions and Marking Guidelines\n",
        "***\n",
        "This assignment will account for 15% of your final grade. It is worth 30 marks in total **(8 for Part 1, 6 for Part 2, and 16 for Part 3).**\n",
        "\n",
        "Please read the provided markdown (text) cells carefully. Some subsections contain multiple questions. Be sure to answer all questions. \n",
        "\n",
        "Answers should be provided within this same notebook and will be a mix of code, outputs (e.g. plots, trained models), and written reponses. \n",
        "\n",
        "#### TIPS\n",
        "\n",
        "\n",
        "> ⏳ **Please budget 4-5 hours to complete each assignment.** Reach out to us if you think your assignment will be more than 1 week late. \n",
        "\n",
        "> 📚 **You will likely need to refer to the documentation of libraries such as sklearn** while completing this assignment. Looking up things in 'the docs' is part of the day-to-day life of a Data Scientist. We encourage you to visit the docs if you haven't already. It's a great way to become more familiar and comfortable with the ML tools available to you. **When in doubt, GOOGLE!** \n",
        "* You can find the scikit-learn docs [here](https://scikit-learn.org/stable/modules/classes.html)\n",
        "* You can also find a variety of helpful guides and tutorials through the [main scikit-learn website](https://scikit-learn.org/stable/)\n",
        "* Other helpful references can be found here:\n",
        " * [MatPlotLib](https://matplotlib.org/)\n",
        " * [Seaborn](seaborn.pydata.org)\n",
        " * [Pandas](https://pandas.pydata.org/docs/)\n",
        " * [Numpy](https://numpy.org/doc/)\n",
        "\n",
        "\n",
        "> 💣  **When you enounter errors** (because it's a law of nature in coding), please carefully read what the error message is telling you. Debugging is a key skill to develop. It's about understanding the root cause of an error, trying a fix, and repeating with a new hypothesis as necessary. If you get totally stuck, try googling the error message. You can find many common errors and solutions documented on [stackoverflow](https://stackoverflow.com/). \n",
        "\n",
        "\n",
        "\n",
        "### Marking Guidelines\n",
        "\n",
        "All questions will state the number of marks available, e.g. [X points].\n",
        "\n",
        "In machine learning, there is not necessarily one \"correct\" answer to a given modelling problem. You **do not** need to have the \"best\" performing model in the class in order to receive full marks. Rather, we will be marking assignments individually, based on the following criteria:\n",
        "\n",
        "* **Demonstration of core concepts [40%]**\n",
        " * Do your answers to written questions demonstrate you understand and can apply the core concepts taught in class? Are those answers justified by the calculations / plots / models you produced?\n",
        "\n",
        "* **Reasonable outputs [30%]** \n",
        " * Do the values you obtain make sense? Did you follow the expected steps. Do you show all steps and make your reasoning clear?\n",
        "\n",
        "* **Effort [15%]** \n",
        " * Are data visualizations or plots clear and easy to interpret? Did you make a reasonable effort to maximize the performance of the models you submitted?\n",
        "\n",
        "* **Clean code [15%]** \n",
        " * Is your python code easy to follow? Related code should be organized into re-usable functions and not be scattered across notebook cells. Function and variable names should be well-chosen to convey what they do or represent. Functions should have docstrings explaining what they do. Comments should be used to explain the details of what you are doing, to help your evaluator follow along. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru0awrdwxFoQ"
      },
      "outputs": [],
      "source": [
        "# === NOTEBOOK & IMPORT SETUP ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from ast import literal_eval\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU-jfbMRmz7s"
      },
      "source": [
        "# Dataset: Research Paper Topic Prediction\n",
        "***\n",
        "In this assignment, you will build a multi-label text classifier to predict the subject areas of arXiv (https://arxiv.org/) papers from their titles and abstract bodies. This type of classifier can be useful for conference submission portals like OpenReview. Given a paper abstract, the portal could provide suggestions for which areas the paper would best belong to.\n",
        "\n",
        "The raw dataset contains 51,774 rows and 3 columns. The columns include:\n",
        "- `titles`: The title of the research paper.\n",
        "- `summaries`: The research paper abstract.\n",
        "- `terms`: The subject area(s) (there can be multiple) for the research paper. These are your labels. Example labels include Machine Learning (cs.LG), Artificial Intelligence (cs.AI), and Cryptography and Security (cs.CR).\n",
        "\n",
        "We perform some light data cleaning below to remove duplicates and process the labels. **The final dataset you will be working with will be stored in the pandas dataframe `df_clean`.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN2Z79wIuBQN"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLTQGPUNuBQN",
        "outputId": "6750151c-434b-434c-e85b-c3e37b23aad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data shape: (51774, 3)\n",
            "There are 12802 duplicate titles.\n",
            "There are 38972 rows in the deduplicated dataset.\n",
            "There are 2 summaries with len<100.\n",
            "There are 38970 rows in the filtered dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries                  terms  \n",
              "0  Stereo matching is one of the widely used tech...         [cs.CV, cs.LG]  \n",
              "1  The recent advancements in artificial intellig...  [cs.CV, cs.AI, cs.LG]  \n",
              "2  In this paper, we proposed a novel mutual cons...         [cs.CV, cs.AI]  \n",
              "3  Consistency training has proven to be an advan...                [cs.CV]  \n",
              "4  To ensure safety in automated driving, the cor...         [cs.CV, cs.LG]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62431433-73b0-4a98-8de6-d7fd7f584bb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>[cs.CV, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>[cs.CV, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>[cs.CV, cs.LG]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62431433-73b0-4a98-8de6-d7fd7f584bb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-62431433-73b0-4a98-8de6-d7fd7f584bb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-62431433-73b0-4a98-8de6-d7fd7f584bb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Load data directly from github\n",
        "df = pd.read_csv(\"https://github.com/soumik12345/multi-label-text-classification/releases/download/v0.2/arxiv_data.csv\")\n",
        "print(f\"Raw data shape: {df.shape}\")\n",
        "\n",
        "# Drop duplicates\n",
        "print(f\"There are {sum(df.titles.duplicated())} duplicate titles.\")\n",
        "df_clean = df[~df[\"titles\"].duplicated()].copy()\n",
        "print(f\"There are {len(df_clean)} rows in the deduplicated dataset.\")\n",
        "\n",
        "# Remove summaries with len < 100\n",
        "print(f\"There are {sum(df_clean.summaries.apply(len)<100)} summaries with len<100.\")\n",
        "df_clean = df_clean[df_clean.summaries.apply(len)>=100].copy()\n",
        "\n",
        "# Remove terms that are present in 25 or less observations\n",
        "df_clean[\"terms\"] = df_clean[\"terms\"].apply(lambda x: literal_eval(x)) # Convert strings to lists\n",
        "all_terms = df_clean.terms.explode().value_counts()\n",
        "valid_terms = list(all_terms[all_terms>=30].index)\n",
        "\n",
        "def validate_terms(row, valid_terms):\n",
        "    return  [i for i in row['terms'] if i in valid_terms]\n",
        "\n",
        "df_clean['terms'] = df_clean.apply(validate_terms, args=[valid_terms], axis=1)\n",
        "df_clean = df_clean[df_clean.terms.apply(len)>=1].copy()\n",
        "print(f\"There are {len(df_clean)} rows in the filtered dataset.\")\n",
        "\n",
        "df_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytwX7-guycO7"
      },
      "source": [
        "# Part 1: EDA [8 points total]\n",
        "***\n",
        "\n",
        "**Tip:** pace yourself. Spend enough time on EDA to get a good grip on the Data, but don't burn all of your time here if it risks not having enough time for the rest of the assignment. You can always return later to improve this section further. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7guiKcEluBQP"
      },
      "source": [
        "## A) Target Inspection [3 points]\n",
        "\n",
        "Let's start off by inspecting the target colum. Your analysis could include (but is not restricted to):\n",
        "- How many unique target values are there? Remember that this is a **multi-label** problem (each observation can have more than one label)!\n",
        "- What does the distribution look like? \n",
        "- Are their any class imbalances? \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJHNgdBbuBQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966a7be7-5958-48c9-f87f-5d2cca271e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " There are 63 unique target values \n"
          ]
        }
      ],
      "source": [
        "## unique target values\n",
        "number = len({x for l in df_clean.terms for x in l})\n",
        "print (f\" There are {number} unique target values \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_dist = df_clean.terms.apply(len);\n",
        "tokens_dist.hist(bins=15);\n",
        "plt.xlabel(\"Number of terms per instance\");\n",
        "plt.ylabel(\"Number of instances\");\n",
        "plt.title(\"Distribution of target variable\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "3IIhOJakayEa",
        "outputId": "bcb01023-ab81-4a95-fdd8-40e190b12ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## distribution\n",
        "# plt.figure(figsize=(25,5))\n",
        "# plt.xticks(rotation=90)\n",
        "# df_clean.terms.hist();"
      ],
      "metadata": {
        "id": "gC8d_3_BqE6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is imbalance in the target class; majority of the instances have inly one term linked to it.This imbalance will affect the accuracy of the multilabel classifier."
      ],
      "metadata": {
        "id": "CdU9EOb56neV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPWtPqTguBQR"
      },
      "source": [
        "## B) Features Inspection [4 points]\n",
        "\n",
        "Now let's dig into the 'feature' columns (`titles` and `summaries`). Your analysis could include (but is not restricted to):\n",
        "- What are the most common words in each feature column?\n",
        "- What is the average word count?\n",
        "- Do word count distributions differ for instances with different 'terms' (labels)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyUgQZzsuBQR"
      },
      "outputs": [],
      "source": [
        "# [YOUR CODE HERE]\n",
        "## common words in each column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbEcbKtSuBQS"
      },
      "source": [
        "## C) Train Test Split [1 point]\n",
        "\n",
        "Perform the usual train-test-validation split. The validation set will be used during training (potentially for early-stopping criteria). The test set will be used to evaluate our models once we are happy with their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKSdoaP-uBQT"
      },
      "outputs": [],
      "source": [
        "# [YOUR CODE HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLCYAXHXuBQT"
      },
      "source": [
        "# Part 2: Data Preprocessing [6 points]\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEm_7jK3uBQT"
      },
      "source": [
        "## A) Multi-Label Encoding [1 point]\n",
        "\n",
        "Alright, now that we have a good handle on the data, lets start to perform some preprocessing to prepare the data for our machine learning models.\n",
        "\n",
        "First you need to multi-hot encode the labels **(we can have more than 1 label per observation, so we use multi-hot as opposed to one-hot label encoding)**. We have already taken care of this for the training data using sklearn's `MultiLabelBinarizer`. All you need to do is **apply the same processing steps to the test and validation sets.** You are encouraged to look at the `MultiLabelBinarizer` docs if you don't understand the outputs of this module.\n",
        "\n",
        "> **NOTE:** The code below assumes you have you train set stored in the variable `train_df`; make adjustments to this variable as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS3j46xzuBQT"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "y_train = mlb.fit_transform(train_df.terms.apply(set))\n",
        "\n",
        "# Should produce a matrix of shape (m, C), where m is # of observations, C is number of classes\n",
        "print(y_train.shape)\n",
        "y_train[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4z5S6F8uBQU"
      },
      "outputs": [],
      "source": [
        "# [YOUR CODE HERE]\n",
        "# y_val = ...\n",
        "# y_test = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VG5TciCuBQU"
      },
      "source": [
        "## B) Text Preprocessing [5 points]\n",
        "\n",
        "Next we need to preprocess our text data. There are two main components to any basic text preprocessing pipeline:\n",
        "\n",
        "1. Perform text cleaning / tokenization. The whole process typically includes the following steps:\n",
        "    - Lowercase\n",
        "    - Remove punctuation, stematize, lematize (all optional)\n",
        "    - Tokenization\n",
        "    - Remove stop words\n",
        "2. Text vectorization. For this you could use a basic count vectorizer or a more sophisticated approach (like TF-IDF).\n",
        "\n",
        "Lucky for us, both of these steps can be taken care of with the use of sklearn's text preprocessing tools, namely `CountVectorizer` and `TfidfVectorizer`. If you look at the docs, you will see that these classes include arguments for the various text preprocessing steps (e.g. `strip_accents`, `lowercase`, `tokenizer`) as well as the vectorization steps (e.g `n_gram_range`, `max_features`). We have provided an example of how this works below. \n",
        "\n",
        "> **NOTE:**  Be sure to perform vectorization for the train, validation, and test sets (but be careful when calling `fit` vs `fit_transform`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RtPMONeuBQU"
      },
      "outputs": [],
      "source": [
        "# Sample vectorization code\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "STOPWORDS = {'should', 'were', 'only', 'i', 'down', 'mightn', 'that', 'and', 'over', 'under', \"wouldn't\", 'now', 'hasn', \"you'll\", 'or', 'ours', 'my', 'does', 'd', 'no', 'because', 'very', 'but', 'such', \"hadn't\", \"haven't\", 'ain', 'yourself', 'weren', 'after', \"hasn't\", 'we', 'not', 'hers', 'how', 'don', 'doesn', 'me', \"you'd\", 'when', \"shouldn't\", 'having', 'he', 'through', 'can', 'themselves', 'whom', 'on', 'them', 'again', 'at', 'if', 'has', 'the', 'wouldn', 'isn', 'do', 'these', \"don't\", \"aren't\", 'too', 'just', 'their', 'most', \"should've\", 'was', 'did', 'yours', 'once', 's', 'from', 'off', 'myself', \"that'll\", 'will', 'doing', 'theirs', \"you're\", 'our', 'have', 'mustn', 'himself', 'of', 'her', 'until', 'more', 'while', 'him', 'there', 'between', 'below', 'your', 'a', 'yourselves', 'herself', 'into', 'who', \"weren't\", 'she', 'they', 'it', 'is', 'all', 'ma', 'both', 'needn', 'as', 'up', 'wasn', 'further', \"didn't\", 'here', 'so', 'by', 'above', 'few', 'had', \"isn't\", 'won', 'an', 'be', 'other', \"couldn't\", 'itself', 'nor', 'hadn', 'any', \"needn't\", \"shan't\", 'own', 'where', 'then', 'shouldn', 'this', 't', \"it's\", 'shan', 'each', \"you've\", 'in', 'are', \"mustn't\", 'which', 'during', 'for', 'same', 'why', 'haven', 'didn', \"won't\", 'what', 'y', 'before', 'being', 'ourselves', 've', 'll', 'out', \"wasn't\", 're', 'some', 'o', 'his', 'those', 'm', 'been', 'to', 'with', 'am', 'against', \"mightn't\", 'than', 'aren', 'couldn', \"she's\", 'its', 'you', \"doesn't\", 'about'}\n",
        "\n",
        "examples = [\n",
        "    \"Hello world! My name is Samantha and I like machine learning\",\n",
        "    \"I like turtles\",\n",
        "    \"One day, the machines will rise to enslave humanity\"\n",
        "]\n",
        "            \n",
        "cv = CountVectorizer(lowercase=True, stop_words=STOPWORDS, max_features=10)\n",
        "examples_vectorized = cv.fit_transform(examples) # outputs spare matrix\n",
        "print(cv.vocabulary_)\n",
        "\n",
        "examples_vectorized.A # use .A to convert sparse to dense array for inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcuC5K8ZuBQU"
      },
      "outputs": [],
      "source": [
        "### [YOUR CODE HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hjTTAXcuBQV"
      },
      "source": [
        "# Part 3: Model Building [16 points]\n",
        "***\n",
        "\n",
        "Alright, we have our dataset and our labels, now its time to train some models! We are going to build 3 neural network models:\n",
        "- Model A): Use vectorized titles data\n",
        "- Model B): Use vectorized abstract data\n",
        "- Model C): Use both titles AND abstract data\n",
        "\n",
        "When building each model, there are a couple things you need to keep in mind:\n",
        "\n",
        "- You may use the keras sequential or functional API, though the sequential API is probably simplest for the first two models.\n",
        "- The model should have **at least 1 hidden layer**, with a non-linear activation function of your choice.\n",
        "- Your hidden layer(s) can have any number of neurons (though typically this ranges from 32 to 512)\n",
        "- This is a multi-class problem, so we *could* use the `softmax` actiavtion function in our final layer. However because this is a multi-class AND multi-label problem, **we are instead going to use the `sigmoid` activation function with 63 output neurons (one for each of the target classes).** This will make evaluation easy; we can simply use the standard \"accuracy\", \"recall\" and \"precision\" metrics (think of this like training 63 binary classification models with a single neural network!)\n",
        "- You will want to feed in the validation data to the `.fit` method to ensure your accuracy/recall is increasing as your loss decreases.\n",
        "- Train for as many epochs as you see fit (or try using an early stopping callback to stop training once validation precision or recall stops increasing).\n",
        "\n",
        "> 💡 Tip: If model training is taking too long and preventing you from completing the assignment in time, then you can do one of two things. Either limit the number of epochs and do your best to optimize within that constraint (preferred). Or reduce the dataset size by taking a random subset of the data (more risky because your network might not have enough data to learn anything useful). Try your best without these restrictions, but know that you have this option as a backup plan, if you are truly running out of time.\n",
        "\n",
        "Note that **recall** in this example is particularily useful, as most observations only have 1-3 labels (meaning that on average the model should *hopefully* predict low probability scores for ~60 classes). We have provided an example of how recall and precision work below, but all you will have to do is insert them into the `model.compile` method as follows:\n",
        "\n",
        "```\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\", tf.keras.metrics.Recall(), tf.keras.metrics.Precision()],\n",
        ")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUZX0kAUuBQV"
      },
      "outputs": [],
      "source": [
        "# Recall/Precision example\n",
        "R = tf.keras.metrics.Recall()\n",
        "P = tf.keras.metrics.Precision()\n",
        "\n",
        "# 3 observations\n",
        "y_true = [\n",
        "    [1,0,0,1,1], # 3 labels\n",
        "    [0,1,0,0,0], # 1 labels\n",
        "    [0,1,1,0,0], # 2 labels\n",
        "]\n",
        "\n",
        "# Sigmoid ouputs\n",
        "y_pred = [\n",
        "    [0.9, 0.1, 0.2, 0.9, 0.4], # recall = 2/3, precision = 2/2 \n",
        "    [0.9, 0.9, 0.1, 0.1, 0.9], # recall = 1/1, precision = 1/3 \n",
        "    [0.1, 0.6, 0.1, 0.3, 0.9], # recall = 1/2, precision = 1/2\n",
        "]\n",
        "\n",
        "# 4 positives predicted correctly / 6 true positives = recall of 0.66\n",
        "print(R(y_true, y_pred))\n",
        "\n",
        "# 4 positives predicted correctly / 7 positives predicted = precision of 0.57\n",
        "print(P(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABVur4WJuBQV"
      },
      "source": [
        "## A) Titles Model [5 points]\n",
        "\n",
        "Lets start by building a model that **uses only titles data** as its input feature (ignore 'summaries' for now). \n",
        "\n",
        "Start with a model that only has 1 hidden dense layer (`tf.keras.layers.Dense`), and then try experimenting with additional layers/neurons/activation functions. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Yu24EUuBQV",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# [YOUR CODE HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSfyBnNuBQV"
      },
      "source": [
        "## B) Summaries Model [3 points]\n",
        "\n",
        "Now lets try train the exact same model, but this time using only the summaries vectorized data. Do not use the 'titles' feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBfeKtqsuBQW",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# [YOUR CODE HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0_uD41OuBQW"
      },
      "source": [
        "## C) Titles and Summaries Model [6 points]\n",
        "\n",
        "Now lets create a model that accepts both inputs! You will need to adjust your model to accept two inputs, and add a concatenation layer that fuses them together in a single tensor. Then send the concatenated input directly through the dense layers just as you did above. \n",
        "\n",
        "> **NOTE:** Remember to include both inputs when calling the `model.fit` method. Also note that that your inputs should be larger than the inputs for the models above. This may slow down training significantly (depending on your hardware), so you may want to try increasing the batch size to speed things up (a batch size of 2024 is probably as high as you want to go)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OqgFrTzuBQW"
      },
      "outputs": [],
      "source": [
        "# [YOUR CODE HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si8Dv7dGuBQW"
      },
      "source": [
        "## D) Evaluate Models [2 points]\n",
        "\n",
        "Evaluate all 3 models on the test set. Which one performed best? Did the third model yield a large enough accuracy boost (if at all) to warrant the increased training time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QodTF7AtuBQW"
      },
      "outputs": [],
      "source": [
        "# [YOUR CODE HERE]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment 4 - Multi-label Text Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}